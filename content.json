{"posts":[{"title":"cuda1","text":"CUDA 介绍CPU ：面向延迟设计image-20210607010547778(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607010547778.png ALU 减少操作延迟 Cache 将长延迟内存访问转换为短延迟缓存访问 控制模块 分支预测以减少分支延迟 数据转发以减少数据延迟 GPU：面向吞吐量设计image-20210607010747373(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607010747373.png 小缓存 提高内存吞吐量 简单控制 无分支预测 无数据转发 高效的ALU 许多、长延迟但大量流水线以实现高吞吐量 需要大量线程来容忍延迟 线程逻辑 线程状态 并行代码，GPU比CPU快10倍以上，串行代码，CPU比GPU快10倍以上 CUDA编程模型编程模型是底层计算机系统的抽象，它允许表达算法和数据结构。语言和 API 提供了这些抽象的实现，并允许将算法和数据结构付诸实践——编程模型的存在独立于编程语言和支持 API 的选择。 一些设计目标 扩展到 100 个内核、1000 个并行线程 让程序员专注于并行算法 不是并行编程语言的机制。 启用异构系统（即 CPU+GPU） CPU 和 GPU 是具有独立 DRAM 的独立设备 关键并行抽象 并发线程的层次结构 轻量级同步原语 协作线程的共享内存模型 线程层次结构 线程thread - 由 CUDA 运行时分发 （由 threadIdx 标识） Warp – 最多 32 个线程的调度单元 块block – 用户定义的 1 到 512 个线程组。 （由 blockIdx 标识） 网格grid – 一组一个或多个块。 为每个 CUDA 核函数创建一个网格 cuda内存层次结构 寄存器 每个线程内存用于自动变量和寄存器溢出。 共享内存 每块低延迟内存，允许块内数据共享和同步。 线程可以通过这块内存安全地共享数据，并且可以通过 _ _syncthreads() 进行屏障同步 全局内存 可以在块或网格之间共享的设备级内存 硬件Tesla 架构的主要组件是：流式多处理器（8800 有 16 个）标量处理器内存层次结构互联网络主机接口 流式多处理器Streaming Multiprocessor (SM)image-20210607012204300(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607012204300.png 每个 SM 有 8 个标量处理器 (SP)Each SM has 8 Scalar Processors (SP) IEEE 754 32 位浮点支持（不完全支持）- IEEE 754 32-bit floating point support (incomplete support) 每个 SP 是一个 1.35 GHz 处理器（32 GFLOPS 峰值）- Each SP is a 1.35 GHz processor (32 GFLOPS peak) 支持 32 位和 64 位整数- Supports 32 and 64 bit integers 8,192 个动态分区的 32 位寄存器- 8,192 dynamically partitioned 32-bit registers 硬件支持 768 个线程（32 个线程的 24 个 SIMT 经线）- Supports 768 threads in hardware (24 SIMT warps of 32 threads) 在硬件中完成的线程调度- Thread scheduling done in hardware 16KB 低延迟共享内存- 16KB of low-latency shared memory 2 个特殊函数单元（平方根倒数、三角函数等）- 2 Special Function Units (reciprocal square root, trig functions, etc) 数据并行 - 向量加法示例image-20210607155913228(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607155913228.png 1234567891011121314// Compute vector sum C = A + Bvoid vecAdd(float *h_A, float *h_B, float *h_C, int n){ int i; for (i = 0; i&lt;n; i++) h_C[i] = h_A[i] + h_B[i];}int main(){ // Memory allocation for h_A, h_B, and h_C // I/O to read h_A and h_B, N elements … vecAdd(h_A, h_B, h_C, N);} cudaMalloc() 在设备全局内存中分配一个对象 两个参数 指向已分配对象的指针的地址 已分配对象的大小（以字节为单位） cudaFree() 从设备全局内存中释放对象 一个参数 指向释放对象的指针 cudaMemcpy() 内存数据传输 需要四个参数 指向目的地的指针 指向源的指针 复制的字节数 转移类型/方向 向量加法主机代码123456789101112131415void vecAdd(float *h_A, float *h_B, float *h_C, int n){ int size = n * sizeof(float); float *d_A, *d_B, *d_C; cudaMalloc((void **) &amp;d_A, size); cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice); cudaMalloc((void **) &amp;d_B, size); cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice); cudaMalloc((void **) &amp;d_C, size); // Kernel invocation code – to be shown later cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost); cudaFree(d_A); cudaFree(d_B); cudaFree (d_C);} cuda执行模式异构主机（CPU）+设备（GPU）应用C程序 主机 C 代码中的串行部分 设备 SPMD 内核代码中的并行部分 image-20210607160519981(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607160519981.png ISA级别的程序 程序是存储在内存中的一组指令，可由硬件读取、解释和执行。 CPU 和 GPU 都是基于（不同的）指令集设计的 程序指令对存储在存储器和/或寄存器中的数据进行操作。 作为冯诺依曼处理器的线程线程是“虚拟化的”或“抽象的” image-20210607160745684(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607160745684.png 并行线程数组 CUDA 内核由线程网格（数组）执行 网格中的所有线程都运行相同的内核代码（SPMD，Single Program Multiple Data） 每个线程都有用于计算内存地址和做出控制决策的索引 image-20210607160922707(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607160922707.png 线程块：可扩展的合作image-20210607160955464(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607160955464.png 将线程数组分成多个块 块内的线程通过共享内存、原子操作和屏障同步进行协作 不同块中的线程不交互 blockIdx and threadIdx 每个线程使用索引来决定要处理的数据 blockIdx：1D、2D 或 3D (CUDA 4.0) threadIdx：1D、2D 或 3D 处理多维数据时简化内存寻址 图像处理 求解体积上的偏微分方程 … image-20210607161116355(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607161116355.png NVCC编译器 NVIDIA 提供了一个 CUDA-C 编译器 nvcc NVCC 编译设备代码，然后将代码转发到主机编译器（例如 g++） 可用于编译和链接host only应用程序","link":"/2019/12/07/CUDA/cuda1/"},{"title":"cuda2","text":"多维内核多维内核配置示例image-20210607161352581(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607161352581.png 处理2D网格的图片C/C++ 中的行优先布局 PictureKernel的源代码12345678910__global__ void PictureKernel(float* d_Pin, float* d_Pout, int height, int width){ // Calculate the row # of the d_Pin and d_Pout element int Row = blockIdx.y*blockDim.y + threadIdx.y; // Calculate the column # of the d_Pin and d_Pout element int Col = blockIdx.x*blockDim.x + threadIdx.x; // each thread computes one element of d_Pout if in range if ((Row &lt; height) &amp;&amp; (Col &lt; width)) { d_Pout[Row*width+Col] = 2.0*d_Pin[Row*width+Col]; }} 用于启动 PictureKernel 的主机代码12345678// assume that the picture is m × n, // m pixels in y dimension and n pixels in x dimension// input d_Pin has been allocated on and copied to device// output d_Pout has been allocated on device…dim3 DimGrid((n-1)/16 + 1, (m-1)/16+1, 1);dim3 DimBlock(16, 16, 1);PictureKernel&lt;&lt;&lt;DimGrid,DimBlock&gt;&gt;&gt;(d_Pin, d_Pout, m, n); 用16x16的块覆盖62x76的图片![image-20210607161715736(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607161715736.png 并非 Block 中的所有线程都将遵循相同的控制流路径。 彩色到灰度图像处理示例RGB图像 图像中的每个像素都是一个 RGB 值 图像行的格式是 (r g b) (r g b) … (r g b) RGB 范围分布不均 RGB转灰度图像灰度数字图像是其中每个像素的值仅携带强度信息的图像。 颜色计算公式 对于 (I, J) 处的每个像素 (r g b)，执行： grayPixel[I,J] = 0.21r + 0.71g + 0.07*b 这只是一个点积 &lt;[r,g,b],[0.21,0.71,0.07]&gt; 常量特定于输入 RGB 空间 RGB转灰度代码123456789101112131415161718192021#define CHANNELS 3 // we have 3 channels corresponding to RGB// The input image is encoded as unsigned characters [0, 255]__global__ void colorConvert(unsigned char * grayImage, unsigned char * rgbImage, int width, int height) { int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; if (x &lt; width &amp;&amp; y &lt; height) { // get 1D coordinate for the grayscale image int grayOffset = y*width + x; // one can think of the RGB image having // CHANNEL times columns than the gray scale image int rgbOffset = grayOffset*CHANNELS; unsigned char r = rgbImage[rgbOffset ]; // red value for pixel unsigned char g = rgbImage[rgbOffset + 2]; // green value for pixel unsigned char b = rgbImage[rgbOffset + 3]; // blue value for pixel // perform the rescaling and store it // We multiply by floating point constants grayImage[grayOffset] = 0.21f*r + 0.71f*g + 0.07f*b; }} 12345678910111213141516171819202122#define CHANNELS 3 // we have 3 channels corresponding to RGB// The input image is encoded as unsigned characters [0, 255]__global__ void colorConvert(unsigned char * grayImage, unsigned char * rgbImage, int width, int height) { int x = threadIdx.x + blockIdx.x * blockDim.x; int y = threadIdx.y + blockIdx.y * blockDim.y; if (x &lt; width &amp;&amp; y &lt; height) { // get 1D coordinate for the grayscale image int grayOffset = y*width + x; // one can think of the RGB image having // CHANNEL times columns than the gray scale image int rgbOffset = grayOffset*CHANNELS; unsigned char r = rgbImage[rgbOffset ]; // red value for pixel unsigned char g = rgbImage[rgbOffset + 1]; // green value for pixel unsigned char b = rgbImage[rgbOffset + 2]; // blue value for pixel // perform the rescaling and store it // We multiply by floating point constants grayImage[grayOffset] = 0.21f*r + 0.71f*g + 0.07f*b; }} 图像模糊模糊框img src=”C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607162547106.png” alt=”image-20210607162547106” style=”zoom: 80%;” /&gt; 2D内核的图像模糊12345678__global__ void blurKernel(unsigned char * in, unsigned char * out, int w, int h) { int Col = blockIdx.x * blockDim.x + threadIdx.x; int Row = blockIdx.y * blockDim.y + threadIdx.y; if (Col &lt; w &amp;&amp; Row &lt; h) { ... // Rest of our kernel } } 12345678910111213141516171819202122232425__global__ void blurKernel(unsigned char * in, unsigned char * out, int w, int h) { int Col = blockIdx.x * blockDim.x + threadIdx.x; int Row = blockIdx.y * blockDim.y + threadIdx.y; if (Col &lt; w &amp;&amp; Row &lt; h) { int pixVal = 0; int pixels = 0; // Get the average of the surrounding 2xBLUR_SIZE x 2xBLUR_SIZE box for(int blurRow = -BLUR_SIZE; blurRow &lt; BLUR_SIZE+1; ++blurRow) { for(int blurCol = -BLUR_SIZE; blurCol &lt; BLUR_SIZE+1; ++blurCol) { int curRow = Row + blurRow; int curCol = Col + blurCol; // Verify we have a valid image pixel if(curRow &gt; -1 &amp;&amp; curRow &lt; h &amp;&amp; curCol &gt; -1 &amp;&amp; curCol &lt; w) { pixVal += in[curRow * w + curCol]; pixels++; // Keep track of number of pixels in the accumulated total } } } // Write our new pixel value out out[Row * w + Col] = (unsigned char)(pixVal / pixels); } } 线程调度 每个块可以相对于其他块以任何顺序执行。 硬件可以随时自由地将块分配给任何处理器 内核可扩展到任意数量的并行处理器 示例：执行线程块 线程以块粒度分配给流式多处理器 (SM) 在资源允许的情况下，每个 SM 最多 8 个块 Fermi SM 最多可以占用 1536 个线程 可以是 256（线程/块）* 6 块 或 512（线程/块）* 3 个块等。 SM 维护线程/块 idx # s SM 管理/调度线程执行 ![image-20210607164925138(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607164925138.png 具有 SIMD 单元的 Von-Neumann 模型![image-20210607164946076(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607164946076.png 作为调度单位的Warp 每个 Block 作为 32 线程 Warps 执行 实施决策，不属于 CUDA 编程模型的一部分 Warps 是 SM 中的基本调度单元 未来的 GPU 可能在每个warp中有不同数量的线程 ![image-20210607165046866(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607165046866.png 线程调度 warp中的线程在 SIMD 中执行 选中时，warp 中的所有线程都执行相同的指令 N 路路径→1/N 吞吐量（应在同一路径内拓展分支） SM实现零开销warp调度 其下一条指令的操作数已准备好供使用的 Warps 有资格执行 根据优先级调度策略选择合格的 Warps 进行执行 ![image-20210607165216727(C:\\Users\\Aerialith\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210607165216727.png warp示例 如果给一个 SM 分配了 3 个块，每个块有 256 个线程，那么一个 SM 中有多少个 Warp？ 每个 Block 分为 256/32 = 8 Warps 有 8 * 3 = 24 个warp 块粒度注意事项 对于使用多个块的矩阵乘法，我应该为 Fermi 使用 8X8、16X16 还是 32X32 块？ 对于 8X8，我们每个块有 64 个线程。 由于每个 SM 最多可以占用 1536 个线程，这相当于 24 个块。 但是，每个 SM 最多只能占用 8 个 Blocks，每个 SM 只能有 512 个线程！ 对于 16X16，我们每个块有 256 个线程。 由于每个 SM 最多可以占用 1536 个线程，因此它最多可以占用 6 个块并实现满容量，除非其他资源考虑无效。 对于 32X32，我们每个块有 1024 个线程。 费米的 SM 中只能容纳一个块。 仅使用 SM 线程容量的 2/3。","link":"/2021/06/07/CUDA/cuda2/"},{"title":"cuda3","text":"内存和数据局部性示例 – 矩阵乘法 #### 一个基本的矩阵乘法 123456789101112131415__global__ void MatrixMulKernel(float* M, float* N, float* P, int Width) { // Calculate the row index of the P element and M int Row = blockIdx.y*blockDim.y+threadIdx.y; // Calculate the column index of P and N int Col = blockIdx.x*blockDim.x+threadIdx.x; if ((Row &lt; Width) &amp;&amp; (Col &lt; Width)) { float Pvalue = 0; // each thread computes one element of the block sub-matrix for (int k = 0; k &lt; Width; ++k) { Pvalue += M[Row*Width+k]*N[k*Width+Col]; } P[Row*Width+Col] = Pvalue; }} GPU 上的性能如何 CGMA ratio（Compute to Global Memory Access ratio）：每次访问CUDA内核区域内全局内存执行的浮点计算次数。 越大越好 基本矩阵乘法的CGMACGMA就是看你取一次数，多少次运算需要用到这个数的值 所有线程为其输入矩阵元素访问全局内存 每次浮点加法一次内存访问（4 字节） CGMA ratio =1 4B/s 的内存带宽/FLOPS 假设一个 GPU 峰值浮点速率 1,500 GFLOPS，200 GB/s DRAM 带宽 4*1,500 = 6,000 GB/s 需要达到峰值 FLOPS 200 GB/s 的内存带宽将执行速度限制在 50 GFLOPS 这将执行率限制为设备峰值浮点执行率的 3.3% (50/1500)！ 需要大幅减少内存访问以接近 1,500 GFLOPS 要达到 1,500 GFLOPS 的峰值，我们需要 CGMA=30 如何提高内存访问效率？ 增加计算量 提高利用率 利用内存层次结构 全局内存 共享内存 寄存器文件 声明CUDA变量 Variable declaration Memory Scope Lifetime int LocalVar; register thread thread device shared int SharedVar; shared block block device int GlobalVar; global grid application device constant int ConstantVar; constant grid application __ device __ 在与 __ shared __ 或 __ constant __ 一起使用时是可选的 自动变量驻留在寄存器中 除了驻留在全局内存中的每线程数组 示例：共享内存变量声明1234void blurKernel(unsigned char * in, unsigned char * out, int w, int h) { __shared__ float ds_in[TILE_WIDTH][TILE_WIDTH]; …} CUDA 中的共享内存 一种特殊类型的内存，其内容在内核源代码中明确定义和使用 每个SM一个 以比全局内存高得多的速度（在延迟和吞吐量方面）访问 访问和共享范围——线程块 Lifetime——线程阻塞，对应线程结束执行后内容会消失 通过内存加载/存储指令访问 计算机体系结构中的一种暂存存储器形式 基本矩阵乘法内核的全局内存访问模式 分块/阻塞 - 基本理念 将全局内存内容划分为tiles 将线程的计算集中在每个时间点的一个或少量分块上 分块的基本概念 在拥堵的交通系统中，显着减少车辆可以大大改善所有车辆看到的延迟 为通勤者拼车 全局内存访问的平铺 司机 = 访问其内存数据操作数的线程 汽车 = 内存访问请求 一些计算对分块更具挑战性 有些拼车可能比其他拼车更容易 拼车参与者需要有类似的工作时间表 有些车辆可能更适合拼车 分块也存在类似的挑战 需要同步： 大纲 识别由多个线程访问的全局内存内容块 将 tile 从全局内存加载到片上内存中 使用屏障同步来确保所有线程都准备好开始阶段 让多个线程从片上存储器访问它们的数据 使用屏障同步来确保所有线程都完成了当前阶段 移动到下一个tile •Tiled Matrix Multiplication Kernel矩阵乘法数据访问模式 每个线程 - 一行 M 和一列 N 每个线程块 – 一条 M 条和一条 N 条 分块矩阵乘法 将每个线程的执行分解成阶段 这样线程块在每个阶段的数据访问都集中在M的一个tile和N的一个tile上 tile在每个维度中包含 BLOCK_SIZE 个元素 加载tile一个块中的所有线程都参与每个线程在平铺代码中加载一个 M 元素和一个 N 元素 屏障同步 同步块中的所有线程 __syncthreads() 同一个块中的所有线程必须到达 __syncthreads() 才能继续前进 最适合用于协调分阶段执行平铺算法 确保在阶段开始时加载tile的所有元素 确保在阶段结束时消耗tile的所有元素 处理分块的边界条件 平铺矩阵乘法内核123456789101112131415161718192021222324__global__ void MatrixMulKernel(float* M, float* N, float* P, Int Width){ __shared__ float ds_M[TILE_WIDTH][TILE_WIDTH]; __shared__ float ds_N[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int Row = by * blockDim.y + ty; int Col = bx * blockDim.x + tx; float Pvalue = 0; // Loop over the M and N tiles required to compute the P element for (int p = 0; p &lt; WIDHT/TILE_WIDTH; ++p) { // Collaborative loading of M and N tiles into shared memory ds_M[ty][tx] = M[Row*Width + p*TILE_WIDTH+tx]; ds_N[ty][tx] = N[(p*TILE_WIDTH+ty)*Width + Col]; __syncthreads(); for (int i = 0; i &lt; TILE_WIDTH; ++i)Pvalue += ds_M[ty][i] * ds_N[i][tx]; __synchthreads(); } P[Row*Width+Col] = Pvalue;} 分块（线程块）大小注意事项 每个线程块应该有多个线程 16 的 TILE_WIDTH 给出 1616 = 256 个线程 32 的 TILE_WIDTH 给出 3232 = 1024 个线程 For 16, in each phase, each block performs 2256 = 512 float loads from global memory for 256 * (216) = 8,192 mul/add operations. (16 floating-point operations for each memory load)CGMA=? For 32, in each phase, each block performs 21024 = 2048 float loads from global memory for 1024 * (232) = 65,536 mul/add operations. (32 floating-point operation for each memory load)CGMA=? 共享内存和线程 For an SM with 16KB shared memory Shared memory size is implementation dependent! For TILE_WIDTH = 16, each thread block uses 22564B = 2KB of shared memory. For 16KB shared memory, one can potentially have up to 8 thread blocks executing This allows up to 8512 = 4,096 pending loads. (2 per thread, 256 threads per block) The next TILE_WIDTH 32 would lead to 232324 Byte= 8K Byte shared memory usage per thread block, allowing 2 thread blocks active at the same time However, the thread count limitation of 1536 threads per SM in current generation GPUs will reduce the number of blocks per SM to one! Each __syncthread() can reduce the number of active threads for a block More thread blocks can be advantageous 对于具有 16KB 共享内存的 SM 共享内存大小取决于实现！ 对于 TILE_WIDTH = 16，每个线程块使用 22564B = 2KB 的共享内存。 对于 16KB 共享内存，最多可能有 8 个线程块在执行 这允许多达 8*512 = 4,096 个待处理负载。 （每个线程 2 个，每个块 256 个线程） 下一个 TILE_WIDTH 32 将导致每个线程块使用 232324 Byte= 8K Byte 共享内存，从而允许 2 个线程块同时处于活动状态 但是，当前一代 GPU 中每个 SM 1536 个线程的线程数限制将使每个 SM 的块数减少到一个！ 每个 __syncthread() 可以减少一个块的活动线程数 更多的线程块可能是有利的 处理任意大小的矩阵–Threads that do not calculate valid P elements but still need to participate in loading the input tiles –Phase 0 of Block(1,1), Thread(1,0), assigned to calculate non-existent P[3,2] but need to participate in loading tile element N[1,2] – –Threads that calculate valid P elements may attempt to load non-existing input elements when loading input tiles –Phase 0 of Block(0,0), Thread(1,0), assigned to calculate valid P[1,0] but attempts to load non-existing N[3,0] 不计算有效 P 元素但仍需要参与加载输入瓦片的线程Block(1,1)的Phase 0, Thread(1,0)，分配计算不存在的P[3,2]但需要参与加载tile元素N[1,2] 计算有效 P 元素的线程可能会在加载输入图块时尝试加载不存在的输入元素Block(0,0)、Thread(1,0) 的第 0 阶段，分配用于计算有效 P[1,0] 但尝试加载不存在的 N[3,0] “简单”的解决方案–When a thread is to load any input element, test if it is in the valid index range –If valid, proceed to load –Else, do not load, just write a 0 –Rationale: a 0 value will ensure that that the multiply-add step does not affect the final value of the output element –The condition tested for loading input elements is different from the test for calculating output P element –A thread that does not calculate valid P element can still participate in loading input tile elements Loading Elements – with boundary check123456789101112138 for (int p = 0; p &lt; (Width-1) / TILE_WIDTH + 1; ++p) { ++ if(Row &lt; Width &amp;&amp; t * TILE_WIDTH+tx &lt; Width) {9 ds_M[ty][tx] = M[Row * Width + p * TILE_WIDTH + tx];++ } else {++ ds_M[ty][tx] = 0.0;++ }++ if (p*TILE_WIDTH+ty &lt; Width &amp;&amp; Col &lt; Width) {10 ds_N[ty][tx] = N[(p*TILE_WIDTH + ty) * Width + Col];++ } else {++ ds_N[ty][tx] = 0.0;++ }11 __syncthreads(); Inner Product – Before and After12345678910++ if(Row &lt; Width &amp;&amp; Col &lt; Width) {12 for (int i = 0; i &lt; TILE_WIDTH; ++i) {13 Pvalue += ds_M[ty][i] * ds_N[i][tx]; }14 __syncthreads();15 } /* end of outer for loop */++ if (Row &lt; Width &amp;&amp; Col &lt; Width) 16 P[Row*Width + Col] = Pvalue; } /* end of kernel */ 一些要点 对于每个线程，条件是不同的 加载 M 元素 加载 N 元素 计算和存储输出元素 对于大矩阵，控制发散的影响应该很小 处理一般矩形矩阵一般来说，矩阵乘法是根据矩形矩阵定义的 j x k M 矩阵乘以 k x l N 矩阵产生 j x l P 矩阵 我们已经介绍了方阵乘法，一种特殊情况 核函数需要泛化处理一般矩形矩阵 Width 参数被三个参数替换：j、k、l 当Width用于指代M的高度或P的高度时，用j代替 当Width用于指代M的宽度或N的高度时，替换为k 当Width用于指代N的宽度或P的宽度时，替换为l","link":"/2021/06/08/CUDA/cuda3/"},{"title":"cuda4","text":"性能Warps and SIMD一个线程block由 32 个线程warp组成扭曲在多处理器上以物理方式并行执行 (SIMD) Warp是调度单位 控制分支 当 warp 中的线程通过做出不同的控制决策而采取不同的控制流路径时，就会发生控制分支 一些采用 then 路径，另一些采用 if 语句的 else 路径 一些线程与其他线程采用不同数量的循环迭代 采取不同路径的线程的执行在当前的 GPU 中被序列化 一个warp中的线程所采用的控制路径一次遍历一个，直到不再存在。 考虑嵌套控制流语句时，不同路径的数量可能很大 控制分支例子当分支或循环条件是线程索引的函数时，可能会出现分歧 具有分歧的内核语句示例：如果 (threadIdx.x &gt; 2) { }这为块中的线程创建了两个不同的控制路径决策粒度 &lt; 扭曲大小； 线程 0、1 和 2 遵循与第一个warp中的其余线程不同的路径 没有发散的例子：如果 (blockIdx.x &gt; 2) { }决策粒度是块大小的倍数； 任何给定warp中的所有线程都遵循相同的路径 控制分支的性能影响 边界条件检查对于并行代码的完整功能和健壮性至关重要 分块矩阵乘法内核有很多边界条件检查 令人担忧的是，这些检查可能会导致性能显着下降 12345678910if(Row &lt; Width &amp;&amp; p * TILE_WIDTH+tx &lt; Width) { ds_M[ty][tx] = M[Row * Width + p * TILE_WIDTH + tx];} else { ds_M[ty][tx] = 0.0;}if (p*TILE_WIDTH+ty &lt; Width &amp;&amp; Col &lt; Width) { ds_N[ty][tx] = N[(p*TILE_WIDTH + ty) * Width + Col];} else { ds_N[ty][tx] = 0.0; 加载M Tiles时的两种块 直到最后一个阶段，其tiles都在有效范围内的块。 方块有部分一直在有效范围之外 控制分支影响分析假设 16x16 tiles和线程块每个线程块有 8 个warp (256/32)假设 100x100 的方阵每个线程将经历 7 个阶段（上限为 100/16） 有 49 个线程块（每个维度 7 个） 加载M tiles的控制分支TYPE1假设 16x16 TILES和线程块每个线程块有 8 个WARP (256/32)假设 100x100 的方阵每个经线将经历 7 个阶段（100/16 的上限） 有42（$67$）个类型1块，总共有336（$842$）条warps它们都有 7 个阶段，因此有 2,352 (336*7) 个WARP阶段经线只有在最后阶段才有控制发散336 个经线阶段有控制分支 7个阶段：每行取七次，最后一次不完整 只考虑Warp不考虑Block不完整：因为Block不完整会导致整个Warps都不取，也就不存在分支 336个阶段：6*7*8*1 6*7个block，每个8个Warp TYPE2类型2：分配加载底部TILES的7个块，共56（$87$）个扭曲它们都有 7 个阶段，所以有 392 ($567$) 个WARP阶段每个类型 2 块中的前 2 个WARP将保持在有效范围内，直到最后一个阶段剩余的 6 个WARP不在有效范围内 所以，只有 14 (2*7) 个经线阶段有控制分支 14个阶段：2*7*1 7个block，每个2个Warp 2个Warp：两个横排，一个横排16个 在大矩阵情况下，对于性能影响很小 控制分支总体影响类型 1： 块：2,352 个warp阶段中的 336 个具有控制分支类型 2： 块：392 个warp阶段中有 14 个具有控制分支性能影响预计小于 12% (350/2,944 或 (336+14)/(2352+14)) Add。加载N个TILEs时控制发散的影响计算有些不同，留作练习 估计的性能影响取决于数据。对于较大的矩阵，影响将显着较小 一般来说，控制发散对大型输入数据集的边界条件检查的影响应该是微不足道的应该毫不犹豫地使用边界检查来确保完整的功能 内核中充满控制流结构的事实并不意味着会出现严重的控制发散 我们将在 Parallel Algorithm Patterns 模块中介绍一些自然会导致控制发散（例如并行缩减）的算法模式 并行规约划分和总结将数据集分成更小的块让每个线程处理一个块使用归约树将每个块的结果汇总为最终答案 将大的问题分解成小的问题，让每个线程负责一个问题，并利用一棵树将结果归约为最终结果。 Reduction Conputation规约将一组输入的数组汇总成一个值，例如： 求最值 求和 积 算法复杂度o(N) 并行求和规约每个线程负责两个值的求和，需要n/2个线程，执行log(n)次。 in-place 不使用辅助变量来转换输入数据结构 一个简单的数据映射线程每个线程负责部分和向量的偶数索引位置（位置责任）每一步后，不再需要一半的线程输入之一总是来自责任地点在每一步中，其中一个输入来自越来越远的距离 12345678910111213__shared__ float partialSum[2*BLOCK_SIZE];unsigned int t = threadIdx.x;unsigned int start = 2*blockIdx.x*blockDim.x;partialSum[t] = input[start + t];partialSum[blockDim.x+t] = input[start + blockDim.x+t];for (unsigned int stride = 1; stride &lt;= blockDim.x; stride *= 2) { __syncthreads(); if (t % stride == 0) partialSum[2*t]+= partialSum[2*t+stride];} 同步是因为需要在进行下一步前，获得上一步的所有结果，下一步的操作数来源是新的 求和完成后，如果Block非常多，宿主代码可以迭代启动另一个内核进行求和；若较少，则可以传回主机 加和，或利用原子操作累加到全局变量中。 优化每次迭代后Warp中真正参与运算的线程很少，资源利用率非常低，在5次之后每个Warp中只有一个线程在运行但却占用了整个Warp的资源 通过改变索引改善，使得部分和压缩在数组的前面位置 在一些算法中，可以改变索引的使用来改善发散行为交换和结合运算符始终将部分和压缩到 partialSum[] 数组中的前面位置保持活动线程连续 更好的核函数1234567for (unsigned int stride = blockDim.x; stride &gt; 0; stride /= 2) { __syncthreads(); if (t &lt; stride) partialSum[t] += partialSum[t+stride];} 内存并行全局内存（DRAM）带宽 DRAM核心阵列组织 DRAM 核心阵列很慢–DDR: Core speed = ½ interface speed –DDR2/GDDR3: Core speed = ¼ interface speed –DDR3/GDDR4: Core speed = ⅛ interface speed DRAM Bursting （突发）通过将N倍位宽的数据加载至缓冲区，随后以N步读出（仅适用于连续地址） 复数Bank时类似 将内存地址划分为几个不同的区域，当一个地址被读取，整个区域被送出。 内存合并因此，当一个Warp中的所有线程执行一个load时，且访问位在同一个突发区域中时，只会发出一个读取 指令，且访问合并。快。若不是这样，就会发出多个请求，并且一些读出的数据被丢弃。 如果数组访问中的索引采用以下形式，则扭曲中的访问是对连续位置的访问 *A[(expression with terms independent of threadIdx.x) + threadIdx.x];*（ 中间英文：具有独立项的表达式）","link":"/2021/06/09/CUDA/cuda4/"},{"title":"Hexo安装","text":"Hexo是什么Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装 安装 nodejs 和 git首先是安装安装 nodejs ，这是使用 Hexo 必备的组件，安装 nodejs 将会同时安装 npm 。 来到 nodejs官网 进行下载。 nodejs 下载完成后，来到 git官网 下载git。 git 能够起到从 github 下载项目并解压，方便使用 它还可以为你把 Blog 项目挂载到 github 上，此后便可以通过互联网访问你的博客，本篇将暂时不会写这个。 安装 Hexo安装完以上两种组件后，我们可以直接来到 Hexo官网 。 官网首先就是安装代码： 1npm install hexo-cli -g 在终端中输入这行代码，Hexo 将会自行完成安装。 基本操作初始化博客文件夹现在你的计算机中选择一个你想要用来存放 Hexo 博客源文件的地方。然后输入以下代码。 1234567891011121314hexo init blog //初始化一个名为 blog 的文件夹，这是你博客源文件存放的地方。//blog可以修改为任意你想要的名称。cd blog// 进入创建并初始化好的博客文件夹npm install// 安装必备的组件hexo server// 启动服务器// 你可以通过打开终端中的链接：http://localhost:4000/ 来打开初始化的博客网站 new12hexo new &quot;post title with whitespace&quot;// 简单示例 使用 new 命令新建一篇新的文章，scaffold 文件夹中有模板，默认使用 post.md 作为初始模板。 此时你就做得到写文章写文章了。对于 markdown 格式的文章，需要使用到 obsidian 、 vscode（ vscode 需要进一步的设置和下载插件来达到预览等功能）等编辑器来编写。 该命令具体的其他参数请参考官方文档。 generate123hexo generatehexo g // 简写 生成静态文件 serverdeploy123hexo deployhexo d //简写 cleanclean1hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。 参考链接Hexo官网： https://hexo.io/zh-cn/","link":"/2023/02/06/Hexo/Hexo%E5%AE%89%E8%A3%85/"},{"title":"cuda6","text":"SCAN前缀和 串行12for(j=1;j&lt;n;j++) out[j] = out[j-1] + f(j); 并行12forall(j) { temp[j] = f(j) }; scan(out, temp); 工作效率低下的扫描内核优化方案 复杂度为nlog(n) 迭代log(n)次，每次运算数量级为n 每次迭代都需要同步已确保输入为最新值 1234567891011121314__global__ void work_inefficient_scan_kernel(float *X, float *Y, int InputSize) { __shared__ float XY[SECTION_SIZE]; int i = blockIdx.x * blockDim.x + threadIdx.x; if (i &lt; InputSize) {XY[threadIdx.x] = X[i];} // the code below performs iterative scan on XY for (unsigned int stride = 1; stride &lt;= threadIdx.x; stride *= 2) { __syncthreads(); float in1 = XY[threadIdx.x - stride]; __syncthreads(); XY[threadIdx.x] += in1; } __ syncthreads(); If (i &lt; InputSize) {Y[i] = XY[threadIdx.x];}} 但仍劣于串行算法 此扫描执行 log(n) 次并行迭代迭代 do (n-1), (n-2), (n-4),..(n- n/2) 添加每个总和：n * log(n) - (n-1)  O(n*log(n)) 工作 更优的算法将输入转化为平衡树，从下至上扫描树至根，根即为所有叶子综合，部分和记录在树中。 1234567891011121314151617// XY[2*BLOCK_SIZE] is in shared memory//上部分for (unsigned int stride = 1;stride &lt;= BLOCK_SIZE; stride *= 2) { int index = (threadIdx.x+1)*stride*2 - 1; if(index &lt; 2*BLOCK_SIZE) XY[index] += XY[index-stride]; __syncthreads();} for (unsigned int stride = BLOCK_SIZE/2; stride &gt; 0; stride /= 2) { __syncthreads(); int index = (threadIdx.x+1)*stride*2 - 1; if(index+stride &lt; 2*BLOCK_SIZE) { XY[index + stride] += XY[index]; } } __syncthreads(); if (i &lt; InputSize) Y[i] = XY[threadIdx.x]; 复杂度规约时执行log(n)次迭代，每次数量从n/2以1/2递减至1，随后反规约迭代log(n)-1次。 等比数列，2(n-1) 工作效率高的内核在缩减步骤中执行 log(n) 次并行迭代迭代做 n/2, n/4,..1 添加总加：(n-1)  O(n) 工作 它在减少后的反向步骤中执行 log(n)-1 并行迭代迭代做 2-1, 4-1, …. n/2-1 添加总加：(n-2) – (log(n)-1)  O(n) 工作 一些讨论对比第一种优化方式绝对性能更好，因为只需要一个方向的规约。 第二种优化方式更理想，效率更高，占用资源更少 大量输入对于两种方案都是将输入分块，得到块总和后再以相同算法扫描 Exclusive Scan第一个元素赋0，其他元素后移即可 OpenACCopen accelerators 为简化在异构CPU/GPU开发而出现 OpenACC设备结构 运行层次向量：包括多个线程以锁步形式工作(SIMD/SIMT) Worker：共同计算一个向量 Gang：一个或多个Workers共享资源 占有率计算GPU 占用率衡量 GPU 计算资源的利用率。 How much parallelism is running / How much parallelism the hardware could run 基本指令(DIRECTIVE)OpenACC loop directive: gang, worker, vector, seqThe loop directive gives the compiler additional information about the next loop. gang – Apply gang-level parallelism to this loop worker – Apply worker-level parallelism to this loop vector – Apply vector-level parallelism to this loop seq – Do not apply parallelism to this loop, run it sequentially Multiple levels can be applied to the same loop, but the levels must be applied in a top-down order. 指令格式#pragma acc directive-name [clause-list] new-line#pragma acc 编译指示名 [子句[ [,] 子句]…] 换行 OpenACC kernels Directive OpenACC Data movement directive OpenACC Loop","link":"/2021/06/11/CUDA/cuda6/"},{"title":"cuda5","text":"直方图parallel histogramA simple parallel histogram algorithm 将输入分成几部分让每个线程获取输入的一部分每个线程遍历其部分。对于每个字母，增加适当的 bin 计数器 输入分区影响内存访问效率 分段分区导致内存访问效率低下 相邻线程不访问相邻内存位置 访问未合并 DRAM 带宽利用率低 解决更改为交错分区所有线程处理元素的连续部分他们都移动到下一部分并重复内存访问被合并 数据竞争发生在读-修改-写过程中，导致无法预计的错误。 多个线程同时操作一些一样的变量，造成了竞争，出错 使用原子操作可以避免。 原子操作由单个硬件指令对存储器位置执行读-修改-写操作，硬件确保当前原子操作完成之前没有其他线程可以 完成读-修改-写操作，会维护一个队列。 12 int atomicAdd(int* address, int val); //原子加 addr+val,写回addr more12345678910Unsigned 32-bit integer atomic addunsigned int atomicAdd(unsigned int* address, unsigned int val); Unsigned 64-bit integer atomic addunsigned long long int atomicAdd(unsigned long long int* address, unsigned long long int val); Single-precision floating-point atomic add (capability &gt; 2.0)float atomicAdd(float* address, float val); 基本直方图内核1234567891011121314__global__ void histo_kernel(unsigned char *buffer,long size, unsigned int *histo) { int i = threadIdx.x + blockIdx.x * blockDim.x; // stride is total number of threads int stride = blockDim.x * gridDim.x; // All threads handle blockDim.x * gridDim.x // consecutive elements whileint alphabet_position = buffer[i] – “a”; if (alphabet_position &gt;= 0 &amp;&amp; alpha_position &lt; 26) atomicAdd(&amp;(histo[alphabet_position/4]), 1); i += stride; }} 原子操作DRAM对 DRAM 位置的原子操作从读取开始，其延迟为数百个周期原子操作以写入同一位置结束，延迟数百个周期在这整个过程中，没有其他人可以访问该位置 延迟决定吞吐量同一 DRAM 位置上原子操作的吞吐量是应用程序可以执行原子操作的速率。特定位置上的原子操作速率受读取-修改-写入序列的总延迟限制，对于全局存储器 (DRAM) 位置通常超过 1000 个周期。这意味着，如果许多线程尝试在同一位置（争用）上执行原子操作，则内存吞吐量将减少到 &lt; 一个内存通道峰值带宽的 1/1000！ Fermi L2缓存 原子操作中等延迟，约为 DRAM 延迟的 1/10在所有块之间共享全局内存原子的“免费改进” 共享内存原子操作非常短的延迟每个线程块私有需要程序员的算法工作（稍后详述）手动编程 私有化副本有缺点创建和初始化需要开销，将私有化写入最终副本需要开销 但访问和串行化代价更小，总体性能提高10倍以上 如果直方图太大无法私有化可以部分私有化 私有化的成本和收益成本创建和初始化私有副本的开销将私人副本的内容累积到最终副本的开销 受益访问私有副本和最终副本时的争用和序列化要少得多整体性能通常可以提高 10 倍以上 直方图的共享内存原子操作（共享内存需要私有化每个线程子集都在同一个块中吞吐量远高于 DRAM (100x) 或 L2 (10x) 原子减少争用——只有同一块中的线程才能访问共享内存变量这是共享内存的一个非常重要的用例！ 一些其他的东西私有化是用于并行化应用程序的强大且常用的技术 私有直方图大小需要很小适合共享内存 如果直方图太大而无法私有化怎么办？有时可以部分私有化输出直方图并使用范围测试转到全局内存或共享内存 一些数据集在局部区域有大量相同的数据值一个简单而有效的优化是每个线程在更新直方图的相同元素时将连续更新聚合为单个更新","link":"/2021/06/10/CUDA/cuda5/"},{"title":"分布式chap-5","text":"消息传递编程进程 启动时指定数量 在整个程序执行过程中保持不变 都执行相同的程序 每个都有唯一的ID号 交替执行计算和通信 消息传递模型的优势 使程序员能够管理内存层次结构 许多架构的可移植性 更容易创建确定性程序 简化调试 消息传递接口历史 1980 年代后期：供应商拥有独特的库 1989 年：在橡树岭国家实验室开发的并行虚拟机 (PVM) 1992 年：开始制定 MPI 标准 1994 年：MPI 标准 1.0 版 1997 年：MPI 标准 2.0 版 今天：MPI 是主要的消息传递库标准 聚合与映射 并行算法的特性 固定任务数 任务之间没有通信 每个任务所需的时间是可变的 查阅映射策略决策树 以循环方式将任务映射到处理器 循环（交错）分配假设 p 个进程每个进程得到每 pth 件工作示例：5道工序和12件工作P0: 0, 5, 10P1：1、6、11P2: 2, 7P3: 3, 8P4：4、9 编程头文件123456789//mpi依赖#include&lt;mpi.h&gt;#include&lt;stdio.h&gt;int main (int argc, char *argv[]) { int i; int id; /* Process rank */ int p; /* Number of processes */ void check_circuit (int, int); argc 和 argv：初始化 MPI 需要它们 运行此程序的每个进程的每个变量的一份副本 初始化1MPI_Init (&amp;argc, &amp;argv); 每个进程调用的第一个 MPI 函数 不一定是第一个可执行语句 允许系统进行任何必要的设置 Communicator通信器：为进程提供消息传递环境的不透明对象MPI_COMM_WORLD默认的通讯器包括所有进程可以创建新的通信器将在第 8 章和第 9 章中执行此操作 确定进程数1MPI_Comm_size (MPI_COMM_WORLD, &amp;p); 第一个参数是Communicator通信器第二个是返回的进程数 进程数序号1MPI_Comm_rank (MPI_COMM_WORLD, &amp;id); 第一个是Communicator第二个是进程id 外部变量1234567int total;int main (int argc, char *argv[]) { int i; int id; int p; … 全局变量分配在静态存储区 工作循环分配12for (i = id; i &lt; 65536; i += p) check_circuit (id, i); 外部功能检查电路具有并行性 它可以是一个普通、串行的函数 关闭MPI1MPI_Finalize(); 在所有其他 MPI 库调用之后调用 允许系统释放 MPI 资源 完整代码编译MPI程序1mpicc -O -o foo foo.c mpicc：用于编译和链接 C+MPI 程序的脚本 -O 优化 -o 可执行文件存放位置 运行1mpirun -np &lt;p&gt; &lt;exec&gt; &lt;arg1&gt; … -np 进程数 可执行文件 命令参数 指定主机进程文件**.mpi-machines** 在home目录列出主机进程的使用顺序 例如：.mpi-machines 文件目录： band01.cs.ppu.eduband02.cs.ppu.eduband03.cs.ppu.eduband04.cs.ppu.edu 运行文件其对应的运行结果一个CPU12345678910% mpirun -np 1 sat 0) 10101111100110010) 01101111100110010) 11101111100110010) 10101111110110010) 01101111110110010) 11101111110110010) 10101111101110010) 01101111101110010) 1110111110111001Process 0 is done 两个CPU1234567891011% mpirun -np 2 sat 0) 01101111100110010) 01101111110110010) 01101111101110011) 10101111100110011) 11101111100110011) 10101111110110011) 11101111110110011) 10101111101110011) 1110111110111001Process 0 is doneProcess 1 is done 三个CPU123456789101112% mpirun -np 3 sat 0) 01101111100110010) 11101111110110012) 10101111100110011) 11101111100110011) 10101111110110011) 01101111101110010) 10101111101110012) 01101111110110012) 1110111110111001Process 1 is doneProcess 2 is doneProcess 0 is done 输出分析 输出顺序仅部分反映并行计算机内部输出事件的顺序 如果进程 A 打印两条消息，则第一条消息将在第二条之前出现 如果进程 A 在进程 B 之前调用 printf，则不能保证进程 A 的消息会出现在进程 B 的消息之前 改进程序 希望找到解决方案的总数 将总和规约适用于这个程序 规约是一个集合的通信 修改 修改函数check_circuit 如果电路满足输入组合，则返回 1 否则返回 0 每个进程保持它找到的可满足电路的本地计数 在 for 循环后执行归约 新的声明和代码1234567int count; /* Local sum */int global_count; /* Global sum */int check_circuit (int, int);count = 0;for (i = id; i &lt; 65536; i += p) count += check_circuit (id, i); MPI_Reduce123456789101112131415int MPI_Reduce (void *operand, /* addr of 1st reduction element */ void *result, /* addr of 1st reduction result */ int count, /* reductions to perform */ MPI_Datatype type, /* type of elements */ MPI_Op operator, /* reduction operator */ int root, /* process getting result(s) */ MPI_Comm comm /* communicator */) MPI_DATATYPE OPTIONS1234567891011MPI_CHARMPI_DOUBLEMPI_FLOATMPI_INTMPI_LONGMPI_LONG_DOUBLEMPI_SHORTMPI_UNSIGNED_CHARMPI_UNSIGNEDMPI_UNSIGNED_LONGMPI_UNSIGNED_SHORT MPI_OP OPTIONS123456789101112MPI_BANDMPI_BORMPI_BXORMPI_LANDMPI_LORMPI_LXORMPI_MAXMPI_MAXLOCMPI_MINMPI_MINLOCMPI_PRODMPI_SUM 调用MPI_Reduce12345678MPI_Reduce (&amp;count, &amp;global_count, 1, MPI_INT, MPI_SUM, 0,//only process 0 will get the result MPI_COMM_WORLD);if (!id) printf (&quot;There are %d different solutions\\n&quot;,global_count); 第二个程序执行结果12345678910111213% mpirun -np 3 seq2 0) 01101111100110010) 11101111110110011) 11101111100110011) 10101111110110012) 10101111100110012) 01101111110110012) 11101111101110011) 01101111101110010) 1010111110111001Process 1 is doneProcess 2 is doneProcess 0 is doneThere are 9 different solutions MPI基准测试程序 MPI_Barrier – barrier synchronization 障碍同步 MPI_Wtick – timer resolution 时钟分辨率 MPI_Wtime – current time 当前时间 wall-clock time VS CPU time（挂钟时间和CPU时间） CPU time (or process time) 是 CPU 用于处理指令的时间量。 用户时间+系统时间 Wall-clock time (or wall time ) 从开始到完成任务所经过的时间。 CPU 时间、I/O 时间和通信通道延迟 基准测试代码123double elapsed_time; …MPI_Init (&amp;argc, &amp;argv); MPI_Barrier (MPI_COMM_WORLD); elapsed_time = - MPI_Wtime(); …MPI_Reduce (…); elapsed_time += MPI_Wtime(); 结果 Processors Time (sec) 1 15.93 2 8.38 3 5.86 4 4.60 5 3.77","link":"/2021/06/03/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fchap-5/"},{"title":"分布式chap-4","text":"第四章 性能指标（加速比和效率） 运行时间（执行时间） 加速 效率 可扩展性 便携性、编程能力等等 超线性加速表达式 固有串行计算 σ(n) 潜在并行计算 (n) 并行开销 κ(n,p) 加速比Ts：串行时间Tp：并行时间 效率Ep = Sp/p 阿姆达尔定律 实例： 局限性：忽略了并行的开销，高估了可实现的加速回顾：将问题大小视为常数显示执行时间如何随着处理器数量的增加而减少 其他角度我们经常使用更快的计算机来解决更大的问题实例让我们将时间视为常数，并允许问题规模随着处理器数量的增加而增加 古斯塔夫定律阿姆达尔和古斯塔夫都忽略了并行开销，所以有另一个定律 Karp-Flatt将并行开销包含考虑，并检测加速模型中忽略的其他开销 进程启动时间 进程同步时间 不平衡的工作量 结构开销 等效率指标 并行系统：在并行计算机上执行的并行程序 并行系统的可扩展性：衡量其随着处理器数量增加而提高性能的能力 可扩展的系统在添加处理器时保持效率 等效率：衡量可扩展性的方法 等效率推导步骤 从加速公式开始 计算总开销 假设效率保持不变 确定顺序执行时间和开销之间的关系 可扩展性功能 假设等效率关系为 n &gt;= f(p) 令 M(n) 表示大小为 n 的问题所需的内存 M(f(p))/p 显示每个处理器的内存使用量必须如何增加才能保持相同的效率 我们称 M(f(p))/p 为可伸缩性函数 可扩展性函数的含义 为了在增加 p 时保持效率，我们必须增加 n 受可用内存限制的最大问题大小，在 p 中是线性的 可扩展性函数显示每个处理器的内存使用量必须如何增长才能保持效率 可扩展性函数常数意味着并行系统是完全可扩展的 复杂度例子 串行算法复杂度 T(n,1) = (n) 并行算法 计算复杂度 = (n/p) 通信复杂度 = (log p) 并行开销 T0(n,p) = (p log p) 弗洛伊德算法 顺序时间复杂度：(n3) 并行计算时间：(n3/p) 并行通信时间：(n2log p) 并行开销：T0(n,p) = (pn2log p) 等效率关系 n3  C(p n2 log p)  n  C p log p M(n) = n2 并行系统可扩展性差 有限差分 每次迭代的顺序时间复杂度：(n2) 每次迭代的并行通信复杂度：(n/p) 并行开销：(n p) 等效率关系 n2  Cnp  n  C p M(n) = n2 该算法具有完美的可扩展性","link":"/2021/06/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fchap-4/"},{"title":"分布式chap-6","text":"埃拉托斯特尼筛法串行算法 列出2以后的所有序列： 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 标出序列中的第一个质数，也就是2，序列变成： 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 将剩下序列中，划摽2的倍数（用红色标出），序列变成： 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 如果现在这个序列中最大数小于等于最后一个标出的素数的平方，那么剩下的序列中所有的数都是质数，否则回到第二步。 时间复杂度：（nlnlnn) 并行性来源 区域划分 将数据分成几部分 将计算步骤和数据关联 每个数组元素一个原始任务 实现并行12345678910Making 3(A) parallelMark all multiples of k between k2 and n to for all j where k2  j  n do if j mod k = 0 then mark j (it is not a prime) endifendfor 123456789Making 3(B) parallelFind smallest unmarked number &gt; ktoMin-reduction (to find smallest unmarked number &gt; k)Broadcast (to get result to all tasks) 聚合的目标 合并任务 降低通信开销 平衡进程之间的计算 数据分解选项 交错（循环） 易于确定每个索引的“所有者” 导致此问题的负载不平衡 堵塞 平衡负载 易于标记倍数 如果 n 不是 p 的倍数，则确定所有者更复杂 块划分选项 想要在 n 不是 p 的倍数时平衡工作量 每个进程得到 n/p上界 或 n/p下界 元素 寻求简单的表达 查找给定所有者的低、高指数 给定索引查找所有者 方式1 让 r = n mod p 如果 r = 0，则所有块的大小相同 别的 前 r 个块的大小为 n/p上界 剩余的 p-r 个块的大小为 n/p下界 方式2 进程i的第一个元素 进程i的最后一个元素 给出元素j，求对应的进程号 比较（数值为运算次数） Operations Method 1 Method 2 Low index 4 2 High index 6 4 Owner 7 4 运算次数最少的选择第二个方法 块分解宏命令1234567#define BLOCK_LOW(id,p,n) ((i)*(n)/(p))#define BLOCK_HIGH(id,p,n) \\ (BLOCK_LOW((id)+1,p,n)-1)#define BLOCK_SIZE(id,p,n) \\ (BLOCK_LOW((id)+1)-BLOCK_LOW(id))#define BLOCK_OWNER(index,p,n) \\ (((p)*(index)+1)-1)/(n)) 本地与全局索引 循环元素 分解影响实现 用于筛分的最大素数是$\\sqrt{n}$ 第一个进程有 n/p下界 元素 如果 p &lt; $\\sqrt{n}$，那么第一个进程包含可以筛掉所有非素数的素数 第一个进程总是广播下一个筛选素数 不需要规约过程 快速标记块分解允许与串行算法相同的标记： j, j + k, j + 2k, j + 3k, … 来代替 对于块中的所有 j如果 j mod k = 0 那么标记 j（它不是素数） 并行算法开发 广播函数MPI_Bcast12345678int MPI_Bcast ( void *buffer, /* Addr of 1st element */ int count, /* # elements to broadcast */ MPI_Datatype datatype, /* Type of elements */ int root, /* ID of root process */ MPI_Comm comm) /* Communicator */ MPI_Bcast (&amp;k, 1, MPI_INT, 0, MPI_COMM_WORLD); 任务通道图 分析 $\\chi$是标记一个单元格所需的时间 串行执行时间是$\\chi$ n ln ln n 广播次数 ：$\\sqrt{n}$ / $ln\\sqrt{n}$ 广播时间：$\\lambda$ 乘 $\\log$p的上界 预计执行时间： 性能 执行串行算法 确定 $\\chi$ = 85.47 纳秒 执行系列广播 确定 $\\chi$ = 250 $\\mu$sec 不同进程数执行时间 Processors Predicted Actual (sec) 1 24.900 24.900 2 12.721 13.011 3 8.843 9.039 4 6.768 7.055 5 5.794 5.993 6 4.964 5.159 7 4.371 4.687 8 3.927 4.222 改进 删除偶数 将计算次数减半 较大的n值可以自由的存储 每个过程都能找到自己筛选的素数 将素数的计算复制到 $\\sqrt{n}$ 消除广播 重组循环 提高缓存命中率 优化比较","link":"/2021/06/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fchap-6/"},{"title":"分布式chap.1","text":"第一章 Architecture大纲（outline） 冯诺依曼模型的修改 分布式硬件 弗林分类法 共享内存系统和分布式存储系统 内存层次结构和缓存一致性 互联网络 分布式软件 输入输出 CPU与存储器分离是冯诺依曼模型的瓶颈 改进冯诺依曼模型： 缓存 虚拟内存 低级并行 缓存基础 缓存：可以比其他一些内存位置用更少的时间访问的内存位置的集合。 CPU高速缓存：CPU可以比其访问主内存更快地访问的内存位置的集合。 CPU高速缓存可以与CPU位于同一芯片上，也可以位于与普通存储芯片相比访问速度更快的单独芯片上。 缓存的想法 使用更广泛的互连，并在单个内存访问中传输更多数据或更多指令。 将数据块和指令块存储在实际上更靠近CPU寄存器的特殊内存中。 哪些数据或指令放cache？ 程序倾向使用物理上接近最近使用的数据和指令。 局部性原理（系统使用更宽的互联结构访问数据和指令） 局部性 - 访问一个位置，然后访问附近的位置 在访问一个内存位置后，程序将在不久的将来（时间局部性）访问附近的位置（空间局部性）。 为了利用局部性原理，系统使用更广泛的有效互连来访问数据和指令。（缓存块或缓存行） ​ 时间局部性 ​ 空间局部性 cache分级： L1：最快最小 L2： L3：最慢最大 Cache查找由L1往下查找，Cache中有信息，则命中；信息没找到，则称缺失；那么程序将会从主存读出查找的信息。 缓存写入策略 CPU往cache写数据，cache中的值和主存中的值不一致。 解决：写直达（writing-through）：写入cache时，立即写入主存。 写回（writing-back）：将数据更新的cache line标记为脏（dirty），然后cache line缓存替换时，dirty的行被写入主存。 Cache映射​ 全相联（fully associative）​ 每个cache line可以放置在cache的任何位置​ 直接映射（directed mapped）​ 每个cache line在cache中有唯一位置​ n路组相联（n-way set associative）折中方案​ 每个cache line可以放置到cache中n个不同区域中的一个 内存中的行能映射到cache中的多个不同位置时，如何决定替换或驱逐哪一行？ 常用最近最少使用方案 虚拟存储器大型程序，数据和指令集主存可能放不下，这时就是用虚拟存储器（虚拟内存）。把当前程序用到的放入主存，暂时不需要的放入辅存。虚拟内存也是对数据块和指令块进行操作，通常称为页。辅存较主存慢非常多，所以也的大小通常比较大。 指令级并行流水线将功能分成多个单独的硬件或功能单元， 多发射一个时钟周期发射多条指令，一般处理器有多个累加器或乘法器以实现多发射 编译时调度，静态多发射 运行时调度，动态多发射，超标量 为了能够多发射，需要用到预测， 硬件多线程线程级并行提供粗粒度的并行性，TLP的线程 对 ILP的指令 硬件多线程任务阻塞，线程快速切换，继续其他任务 同步多线程细粒度多线程变种，允许多个线程同时使用多个功能单元来利用超标量处理器的性能。优先线程：有多条指令就绪的线程，能减轻线程减速的问题。 并行硬件SISDSIMD（好单指令多数据，一个控制单元和多个ALU，每个ALU要么在当前数据上执行同一个指令，要么空闲，下一条指令前需等待广播；通过在处理期间划分数据以提高并行性 向量处理器 对数组或数据向量操作；向量寄存器：存储多个操作数组成的向量，并能对其内容进行操作向量化和流水化的功能单元：相同操作应用于向量中的每个元素向量指令：向量上的操作交叉存储器：内存系统有多个内存体组成，访问一个内存体后，想再访问他有延迟，访问其他的内存体则很快步长式存储器访问和硬件散射/聚集操作：固定步长访问向量元素；对无规律间隔的数据进行读（聚集）、写（散射） 优点： 速度快，易于使用 向量编译器擅长识别向量化代码 识别不能向量化的循环并提供不能向量化的原因 高内存带宽 每个加载的数据都使用 缺点： 可扩展性有限 不能处理不规则的数据结构 MISD MIMD（好多指令多数据，包括一组完全独立的处理单元或核，每个处理单元或核都有自己的控制单元和ALU；通常是异步的，即各个处理器能按照他们的节奏运行；没有全局时钟，不同处理器的系统时间没有联系，除非强制同步。 共享内存系统一个或者多核处理器，通过互联网络，与内存系统相互连接，处理器可以访问每个内存区域，处理器之间隐式通信。通常每个核都拥有私有的L1cache，其他级cache可能共享也可能不共享； 一致内存访问 互连网络将所有处理器直接连接到主存，每个处理器访问内存任意区域的时间相同 非一致内存访问 互连网络将每个处理器连接到一块内存，通过处理器中的硬件达到访问内存中其他块的目的，0号处理器访问直接连接的0号内存速度快，0号处理器访问1号处理器直接连接的1号内存速度慢。 分布式内存系统分布式内存系统又称集群：由一组商品化系统组成，通过商品化网络连接（以太网）。每个系统中的处理器连接系统内的自己的内存，每个系统通过互连网络进行显式通信。 互连网络对两种内存系统有影响 共享内存互连网络 共享内存系统，最常用总线和交叉开关矩阵 总线 ​ 交叉开关矩阵 ​ 使用交换器控制数据传递，线表示双向链路。允许在不同设备间通信，速度比总线快；开销相对较高，成本也比较高。 分布式内存互连网络 直接互连 交换器处理器一对一，交换机之间相互连接，形成一个环或者环面网格。比总线高级，因为允许多个通信同时发生。环面网格比环成本高，因为交换器更复杂。 ​ 等分宽度：​ 衡量同时通信的链路数目或连接性的标准，计算链路数量 ​ 链路带宽：​ 传输数据的速度 ​ 等分带宽：​ 衡量网络质量，计算链路带宽。 最理想直接互联网络是：全相连网络 等分宽度：p^2/4 节点数量多不可能做这样的连接，因为他总共需要 p^2/4+p/2 条链路，不切实际。 所以一般作为衡量其他互连网络的基础 超立方体： 用于实际系统中的高度互连的直接互连网络，递归构造。 一维超立方体：全互连系统；二维超立方体：由两个一维组成，通过相应交换器相连，形成一个正方形；n维超立方体有p=2^n个节点，每个节点和n个节点相连，等分宽度为p/2，连接性更高，但需要更强大交换器，以支持连线数量。 编号方法：直接相连的节点，编号只改变一位，例如000连001、010、100。 间接互连 通常由单向连接和一组处理器组成，每个处理器有一个输入链路和一个输出链路，链路通过交换网络连接 ​ 交叉开关矩阵：​ 类似共享内存系统中的交叉开关矩阵，但把双向链路改为单向链路，使用p^2个交换器。 ​ omega网络：​ 交换器是2x2的交叉开关矩阵。成本较交叉开关网络低。共用了**1/2plog2^(p)个交换器，因为2x2，所以总共是2plog2^(p)**个。 ​ 胖树： 延迟与带宽消息传送时间 = l + n / b 延迟（latency）：发送源开始传输数据到目的地接收数据的时间。 带宽（bandwidth）：目的地开始接收数据的速度。 内存结构层次多个层次，cpu内cache有多级，内存延迟， Cache一致性每个CPU有自己的Cache，所以在进行对数据的修改时，会遇到一致性的问题，过时的数据仍存储在CPU中。如果多CPU对一个Cache则不会，但因为是串行访问cache，速度会慢。 一致性协议直写和写回 处理器A和处理器B，各自本地的Cache Line有同一个变量的拷贝，此时该Cache Line处于Shared状态。处理器A在本地修改了变量，除了把本地变量的Cache Line改为Modified状态外，还需要在处理器B读同一个变量前把处理器B存这个变量的Cache Line改为Invalid状态。后续处理器B需要对这个变量读写时，会Cache Miss。会重新从内存拷贝新的数据到CacheLine中。 监听Cache一致性协议想法基于总线系统，监听总线，看到变量更新，则广播，并标记过时的变量为非法。由于每次监听到改变都要广播，广播开销很大。 基于目录的Cache一致性协议通过目录的数据结构，目录存储每行的状态，对应每个Cache Line，更新Cache，对应行的目录项就会更新。当变量要更新时，就会查目录，并将包含这个变量的cache置为非法状态。 伪共享多个CPU的多个线程同时修改自己的变量。。。 并行软件软件是负担， SPMD（single program,multiple data）：仅包含一段可执行代码，通过使用条件转移语句，让这段代码在执行时表现得像是在不同处理器上执行的程序。 进程或线程的协调 分配任务，每个进程/线程分配大致相同的工作量，且使得通信量最小 安排进程/线程之间的同步 安排进程/线程之间的通信 共享内存动态线程共享内存程序使用。主线程等待任务，派生出新的线程，线程处理结束，被终止合并回主线程 静态线程派生所有的线程，工作结束前，所有线程都在运行，所有线程合并回主线程后，主线程做清理工作（释放内存），然后终止。 静态线程比动态线程性能更好，但可能会浪费系统资源（对资源的利用不是很高效） 非确定性定义：给定的输入产生不同的输出。MIMD中，异步运行可能引发。 竞争条件 临界区 互斥 互斥锁（增强了临界区串行性） 忙等待 分布式内存消息传递1234567891011char message [ 1 0 0 ] ;. . .my_rank = Get_rank ( ) ;if ( my_rank == 1) { sprintf ( message , &quot;Greetings from process 1&quot; ) ; Send ( message , MSG_CHAR , 100 , 0 ) ;} else if ( my_rank == 0) { Receive ( message , MSG_CHAR , 100 , 1 ) ; printf ( &quot;Process 0 &gt; Received: %s\\n&quot; , message ) ;} 程序段是SPMD，两个程序使用相同的可执行代码，但执行不同的操作。执行的操作以来与他们的序号。 不同进程中，变量指的是不同的内存块 单向通信 单向通信或远程内存访问中，单个处理器调用一个函数 能够简化通信，大大降低通信成本 实践难以实现 划分全局地址空间的语言（PGAS语言）允许用户使用共享内存技术来对分布式内存硬件进行编程。 私有变量在运行程序的核的局部内存空间中分配，共享数据结构中数据的分配由程序员分配。 输入与输出并行程序输入输出时，制定一些规则 在分布式内存程序中，只有进程0能够访问stdin。共享内存程序中，只有主线程可以访问。 分布式内存和共享内存系统中，所有进程/线程都可以访问stdout和stderr 因为输出到stdout的非确定顺序，一般只有一个进程/线程回将结果输出到stdout。输出调试程序的结构是例外，它可以多个进程/线程写到stdout 只有一个进程/线程你会尝试访问一个除stdin、stdout或者stderr以外的文件。例如每个进程/线程能打开自己私有的文件进行读/写，但没有两个进程/线程能打开相同的文件。 调试程序输出在生成输出结果时，应该包括进程/线程的序号或进程标识符。","link":"/2021/05/30/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fchap-1/"},{"title":"分布式chap-3","text":"第三章 并行程序设计Foster 方法划分、通信：处理与机器无关的问题，影响并发性和可扩展性聚合、映射：处理与机器有关的问题，影响局部性和其他性能问题 划分将要执行的指令和数据按计算部分拆分成多个小任务。关键在于识别出可以并行执行的任务 通信确定划分的任务之间需要执行哪些通信 聚合将前面确定的任务与通信结合成更大的任务。例如任务A必须在任务B之前执行，那么把他们聚合成一个简单符合任务将会比较明智 映射将聚合好的任务分配到进程/线程中。要使通信量最小化，是各个进程/线程分配的工作量大致均衡 划分区域划分 划分数据尽可能等大 生成多个任务，每个都有一些数据和数据操作集 一些经验：首先关注最大的数据结构或最常访问的数据结构 功能划分 区域划分的补充 任务的数据需求可能脱节或者明显重叠 功能划分通常通过流水线实现并发任务集合 区域划分最自然，如果首先用功能划分，则并行算法作为整体会更简单 通信局部通信 每个任务仅与少量的相邻任务通信 创建说明数据流的通道 全局通信 大量相邻任务和远程任务都提供数据来执行计算 通常在设计早期，不为他们创建通道 可能导致通信过多或限制并发执行的机会 纯本地通信的算法中，有两个问题可能会阻碍并行执行： 算法是集中的：他不分布计算和通信，单个任务必须参与每个操作 算法是顺序的：不允许多个计算和通信操作同时进行 常用方法： ​ 分布式通信与计算 ​ 分而治之 通信方式1分布式通信与计算 一种求和算法，N个任务相连，以便对分布在这些任务中的N个数字求和 单个求和仍需N-1步，但只有当多个求和操作要执行是，才允许并发执行 分而治之 划分成两个或多个大小大致相同的较简单问题。 划分产生的子问题能够并行解决时，分治算法是有效的 logN步 通信方式2结构化通信 一个任务和他邻近的任务形成一个规则结构，比如树或网格 非结构化通信 网络可以是任意图 使聚合和映射复杂化 如果通信需求是动态的，则程序运行时需频繁应用负载平衡算法，并且必须权衡这些算法的成本与收益 通信总结 任务间通信是并行算法开销的一部分 最小化并行开销是并行算法设计的一个重要目标 聚合聚合时，考虑合并是否有用，并确定复制数据和/或计算是否值得 目标 提高性能 保持程序可扩展性 简化编程 提高性能可以降低通信成本，a：消除通信；b：减少通信次数 更少的任务创建成本和任务调度成本 可扩展性8x128x256，2、3维聚合，有一4CPU的处理器，可以分为2x128x256来执行；8个CPU，分1x128x256；超过8个CPU，可移植性受损害 讨论 关于聚集和复制，这三个目标导向的决策有时会产生冲突。 通过增加计算和通信力度以降低通信成本，在可伸缩性和映射的决策方面保持灵活性，并降低软件的工程成本 增加粒度 通过发送更少的数据减少通信时间。即使发送相同数量的数据，也可通过使用更少的消息来实现 还可以降低任务创建成本 方法：表面对体积效应；复制计算；避免通信 表面对体机效应 一个任务的通信需求与它所操作的子域的表面成正比，而计算需求与子域的体积成正比。 复制计算 保持灵活性 为了具有可移植性和可扩展性，创建不同数量的任务的能力很重要。 在为特定计算机调整代码时，这种灵活性也很有用 如果任务在等待远程数据时经常阻塞，那么将多个任务映射到一个处理器是有益的 重叠计算和通信：一个任务的通信和另一个任务的计算重叠 创建比处理器更多的任务为在可用处理器上平衡计算负载的映射策略提供了更大的范围 降低软件工程成本 并行化现有的代码时，另一个问题是与不同划分策略相关的相对开发成本 从这个角度看，最有趣的策略可能是那些避免大量代码修改的策略 映射分配任务给处理器的过程。 开发映射算法的目标通常是最小化总执行时间。 冲突目标： 最大化处理器利用率，即系统处理器积极解决问题所需任务的平均时间百分比 最小化处理器间通信 使通信次数最少，并且每个进程/线程得到大致相同的工作量 最佳映射： 寻找最佳映射是一个NP-Hard 必须依赖于启发式 映射决策树静态任务数 结构化通信 每个任务的恒定计算时间 最小化通信的方式聚合任务 每个处理器创建一个任务 每个任务的可变计算时间 循环映射任务到处理器 非结构化通信 使用静态负载平衡算法 动态任务树 任务间频繁通信 使用动态负载平衡算法 许多短期任务（没有任务间通信） 使用运行时任务调度算法 案例边界值问题 PPTchap.3 49页最大值问题 74页n体问题 82页添加数据输入 90页","link":"/2021/06/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fchap-3/"},{"title":"一些心里所想","text":"","link":"/2021/06/01/%E7%A7%98%E5%AF%86/%E4%B8%80%E4%BA%9B%E5%BF%83%E9%87%8C%E8%AF%9D/"},{"title":"分布式chap-7","text":"弗洛伊德算法大纲 全对最短路径问题 动态二维数组 并行算法设计 点对点通信 块行矩阵 I/O 分析和基准测试 全对最短路径 算法1234567for k = 0 to n-1 for i = 0 to n-1 for j = 0 to n-1 a[i,j] = min (a[i,j], a[i,k] + a[k,j]) endfor endforendfor 动态二维数组动态一维数组建立 动态二维数组建立 分布式算法设计 划分 通信 聚合与映射 划分区域划分？功能划分？ 看伪代码 执行相同的赋值语句 $n^3$ 次 没有并行性的函数 区域分解：将矩阵A划分为$n^2$个元素 每个a[i，j]代表一项任务-需要找到一个最短的距离。 然而，要找到距离需要看看所有k的a[i，k]和a[k，j] 1代码同第一个代码块 通信 聚合与映射 任务数：静态 任务间的通信：结构化 每个人物的计算时间：恒定 计划 聚合任务以最大程度地减少通信 每个MPI进程创建一个任务 两种数据划分 比较逐列分块 消除了列内广播 逐行分块 消除行内广播 从文件中读取矩阵更简单 选择逐行分块如何输入一个邻接矩阵假设矩阵在文件中一排排地存储。方法1） 每个进程都读取自己的行（或行）初始数据。该过程必须寻求正确的位置共享文件。方法2） 主进程读取所有行并发送数据到适当的进程。 为什么我们不一次性输入整个文件，然后将其内容分散到各个进程中，以允许并发消息传递?（采用MPI_ScatterMPI_Scatterv） MPI组通信和点到点通信的一个重要区别，就在于它需要一个特定组内的所有进程同时参加通信而不是像点到点通信那样只涉及到发送方和接收方两个进程。组通信在各个不同进程的调用形式完全相同，而不像点到点通信那样在形式上就有发送和接收的区别。 组通信一般实现三个功能：通信、同步和计算。通信功能主要完成组内数据的传输，而同步功能实现组内所有进程在特定的地点在执行进度上取得一致，计算功能要对给定的数据完成一定的操作，例如加减等。所以MPI_Send会具有更好的效率 方法 2）最大限度地减少内存使用，因为只有一个进程需要读取和发送数据。消除了方法1中的寻求过程 ）。 点对点通信 涉及一对进程 一个进程发送消息 其他进程收到消息 不集体发送接收数据 MPI_Send函数12345678int MPI_Send ( void *message, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm) MPI_Recv函数123456789int MPI_Recv ( void *message, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status) MPI_Send和MPI_Recv 内部 MPI_Send函数返回 函数直到消息缓冲区为空才会返回 以下情况缓存区空闲： 消息被复制到了系统缓冲区 消息被传输走了 典型场景 消息复制到系统缓冲区 传输重叠计算 MPI_Recv函数返回 函数直到消息存在与缓冲区才返回 消息永远没到，则函数永远不返回 死锁死锁：进程等待一个永远不会成立的条件 编写简单的发送接收代码 两个进程：都先接收再发送 发送标签与接收标签不匹配 进程向错误的目标进程发送消息 例1： 123456789if (id == 0) { MPI_Recv (&amp;b,...); MPI_Send (&amp;a,...); c = (a + b)/2.0; } else if (id == 1) { MPI_Recv (&amp;a,...); MPI_Send (&amp;b,...); c = (a + b)/2.0; } 进程0等待1的消息而阻塞，进程1等待0的消息而阻塞，产生了死锁。 例2： 123456789if (id ==0) { MPI_Send(&amp;a, ... 1,MPI_COMM_WORLD); MPI_Recv(&amp;b, ... 1, MPI_COMM_WORLD,&amp;status); c = (a+b)/2.0; }else if (id ==1) { MPI_Send(&amp;a, ... 0,MPI_COMM_WORLD); MPI_Recv(&amp;b, ... 0, MPI_COMM_WORLD,&amp;status); c = (a+b)/2.0;} 这两个进程在尝试接收之前都会发送，但仍然陷入死锁。 为什么？ 答：标签错了。进程0正在尝试接收1的标签，但进程1正在发送0的标签。 MPI程序的安全 依赖MPI缓冲区的程序被认为是不安全的 这样的程序对于各种输入集可能运行没有问题，但它可能会挂起或崩溃与其他集。 MPI标准允许MPI_Send以两种不同的方式运行： 他可以简单地将消息复制到MPI管理的缓冲区中并返回 他也可以阻塞，直到对MPI_Recv地匹配调用开始 MPI的许多实现设置了系统从缓冲切换到阻塞的阈值 相对较小的消息将由MPI_Send缓冲 较大的消息将导致他阻塞 例1：从进程0发送一个很大的消息到进程1，如果目的地存储空间不足，发送必须等待用户提供内存空间（通过接收），下列执行顺序会发生什么？ Process 0 Process 1 Send（1） Send（0） Recv（1） Recv（0） 这是“不安全”的，因为它依赖于系统缓冲区的可用性，需要Process 0暂时阻塞等待Process 1提供足够的内存。 例2：生产者和消费者，如果进程 0 需要向处理 1 发送大量消息，并且发送速度比接受速度快。 Process 0 Process 1 Send（1） Recv（0） 在一开始的时候，系统的缓冲区并未被填满，后来满了之后，需要Process 0暂时阻塞等待Process 1提供足够的内存。 方法一：MPI_SsendMPI 标准定义的MPI_Send替代方法。额外的”s”表示同步，MPI_Ssend保证阻塞，直到匹配接收开始。 12345678int MPI_Ssend(){ void * msg_buf_p; int msg_size; MPI_Datatype msg_type; int dest; int tag; MPI_Comm communicator;} 方法二：调整通信12345678910111213MPI_Send(msg,size,MPI_INT,(my_rank+1)%comm_sz,0,comm)MPI_Recv(new_msg,size,MPI_INT,(my_rank+comm_sz-1)%comm_sz,0,comm,MPI_STATUS_IGNORE) 改为if(my_rank%2==0){ MPI_Send(msg,size,MPI_INT,(my_rank+1)%comm_sz,0,comm); MPI_Recv(new_msg,size,MPI_INT,(my_rank+comm_sz-1)%comm_sz,0,comm,MPI_STATUS_IGNORE);}else{ MPI_Recv(new_msg,size,MPI_INT,(my_rank+comm_sz-1)%comm_sz,0,comm,MPI_STATUS_IGNORE); MPI_Send(msg,size,MPI_INT,(my_rank+1)%comm_sz,0,comm);} 偶数个comm_sz没问题，奇数个则不安全 方法三：MPI_Sendrecv 自己安排通信的替代方法。 在单个调用中执行阻塞发送和接收。 dest 和 source 可以相同或不同。 特别有用，因为 MPI 会安排通信以便程序不会挂起或崩溃。 1234567891011121314int MPI_Sendrecv( void* send_buf_p; int send_buf_size; MPI_Datatype send_buf_type; int dest; int send_tag; void* recv_buf_p; int recv_buf_size; MPI_Datatype recv_buf_type; int source; int recv_tag; MPI_Comm communicator; MPI_Status* status_p; ); 集群通信和点对点通信 在通讯子中全部进程必须调用相同的集合函数 例如，如果一个程序试图将一个进程上对MPI Reduce的调用与另一个进程上对MPI Recv的调用相匹配，那么这个程序很可能会挂起或崩溃。 每个进程传递给 MPI 集体通信的参数必须是“兼容的”。 例如，如果一个进程传入 0 作为 目标进程 另一个传入 1，然后调用结果 MPI_Reduce 是错误的，而且程序很可能会挂起或崩溃。 out_data_p参数只会被被dest_process使用 但是，所有的进程仍然需要传入一个对应的实参 out_data_p，即使它只是空值. 点对点通信在 标签和通信子的基础上被匹配。 集群通信不需要使用标签 集群通信不使用标签 他们被匹配仅仅在通信子的基础和他们被调用的顺序 块行剧真I/O并行代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748int main (int argc, char *argv[]){ dtype** a; /*doubly-subscripted array*/ dtype* storage; /*Loacl portion of array elements*/ int i,j,k; int id; int m;/*rows in maxtrix*/ int n;/*columns in maxtrix*/ int p;/*number of processes*/ double time,max_time; void compute_shortest_paths(int,int,int**,int); MPI_Init (&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD,&amp;id); MPI_Comm_size(MPI_COMM_WORLD,&amp;p); read_row_striped_matrix(argv[1],(void*) &amp;a),(void*) &amp;storage,MPI_TYPE,&amp;m,&amp;n,MPI_COMM_WORLD); if(m!=n) terminate(id,&quot;Matrix must be square\\n&quot;); pirnt_row_striped_matrix((void**) a,MPI_TYPE,0,MPI_COMM_WORLD); MPI_Barrier(MPI_COMMON_WORLD); time =-MPI_Wtime; compute_shortest_paths(id,p,(dtype**) a,n); time+=MPI_Wtime; MPI_Reduce(&amp;time,&amp;max_time,1,MPI_DOUBLE,MPI_MAX,0,MPI_COMM_WORLD); if(!id) printf(&quot;Floyd,matrix size %d,%d processes:%6.2f seconds\\n&quot;,n,p,max_time); pirnt_row_striped_matrix((void**) a,MPI_TYPE,0,MPI_COMM_WORLD); MPI_Finalize();}void compute_shortest_paths(int id,int p,dtype** a,int n){ int i,j,k; int offset;//local index of broadcast row int root;//process controlling row to be cast; int *tmp;//holds the broadcast row; tmp=(dtype *)malloc (n*sizeof(dtype)); for(k=0;k&lt;n;k++){ root=BLOCK_OWNER(id,p,n); if(root==id){ offset=k-BLOCK_LOW(id,p,n); for(j=0;j&lt;n;j++){ temp[j]=a[offset][l]; } MPI_Bcast(tmp,n,MPI_TYPE,root,MPI_COMM_WORLD); for(i=0;i&lt;BLOCK_SIZE(id,p,n);i++){ for(j=0;j&lt;n;j++){ a[i][j]=MIN(a[i][j],a[i][k]+temp[j]); } } } free (tmp); }} 分析和基准测试程序计算复杂度 最内层循环具有复杂性 Θ(n) 中间循环最多执行 ↑[n/p]次 外循环执行 n 次 总体复杂度 Θ(n3/p) 空间复杂度 内循环无通信 中间循环无通讯 外循环广播 — 复杂性是 Θ（$nlogp$） 总体复杂度 ($n^2logp$) 执行表达式 表达式1 计算/通信 重叠 表达式2 预测与实际性能对比 Execution Time (sec) Processes Predicted Actual 1 25.54 25.54 2 13.02 13.89 3 9.01 9.60 4 6.89 7.29 5 5.86 5.99 6 5.01 5.16 7 4.40 4.50 8 3.94 3.98","link":"/2021/06/06/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fchap-7/"},{"title":"素描课","text":"第一张 第二张 第三张 第四章 第五张 第六张 第七张 第八张 第九张 第十张 期末考","link":"/2021/05/26/%E5%90%83%E5%96%9D%E7%8E%A9%E4%B9%90/%E7%B4%A0%E6%8F%8F%E8%AF%BE/"}],"tags":[{"name":"CUDA","slug":"CUDA","link":"/tags/CUDA/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"复习","slug":"复习","link":"/tags/%E5%A4%8D%E4%B9%A0/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"素描","slug":"素描","link":"/tags/%E7%B4%A0%E6%8F%8F/"},{"name":"绘画","slug":"绘画","link":"/tags/%E7%BB%98%E7%94%BB/"}],"categories":[{"name":"Hexo Blog","slug":"Hexo-Blog","link":"/categories/Hexo-Blog/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"吃喝玩乐","slug":"吃喝玩乐","link":"/categories/%E5%90%83%E5%96%9D%E7%8E%A9%E4%B9%90/"}],"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"碎碎念","text":"tips：github登录后按时间正序查看、可点赞加❤️、本插件地址..「+99次查看」 碎碎念加载中，请稍等… $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ clientID: 'fff19f1ed037cc7e0d9c', clientSecret: 'c88201a14042a3ae4dc306cb616a5e35e953a2a5', id: '625340', repo: 'self-talking', owner: 'Erial21', admin: \"Erial21\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container'); }); %%script src=”/js/gitalk_self.min.js”&gt;/script&gt;script&gt;var gitalk = new Gitalk({id: “d64e1b6440d8ef816848d79226db7cba”,repo: “self-talking”,owner: “Erial21”,clientID: “fff19f1ed037cc7e0d9c”,clientSecret: “c88201a14042a3ae4dc306cb616a5e35e953a2a5”,admin: [“Erial21”],createIssueManually: false,distractionFreeMode: true,perPage: 20,pagerDirection: “last”,enableHotKey: true,language: “zh-CN”,})gitalk.render(‘comment-container1’)&lt;/script%%","link":"/self-talking/index.html"}]}